{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– SENTALIS â€” Sentiment Analysis\n",
    "## TF-IDF + Naive Bayes (Machine Learning Edition)\n",
    "\n",
    "**Input:** Dataset berlabel dari SENTALIS Rule-Based (`dataset_ml.csv`)  \n",
    "**Output:** Model terlatih + laporan evaluasi lengkap\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ—ºï¸ Alur Notebook\n",
    "```\n",
    "1. Load & eksplorasi dataset\n",
    "2. Analisis distribusi kelas (imbalanced data)\n",
    "3. Feature engineering dengan TF-IDF\n",
    "4. Training Naive Bayes (Multinomial & Complement)\n",
    "5. Evaluasi model â€” accuracy, precision, recall, F1\n",
    "6. Confusion matrix & visualisasi\n",
    "7. Analisis kesalahan prediksi\n",
    "8. Prediksi teks baru\n",
    "9. Simpan model\n",
    "```\n",
    "\n",
    "> ðŸ“ Pastikan file `dataset_ml.csv` atau `dataset_train.csv` / `dataset_test.csv`  \n",
    "> tersedia di direktori yang sama, atau upload ke Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Cell 1 â€” Install & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas matplotlib seaborn imbalanced-learn joblib -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn: preprocessing & model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report,\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Imbalanced-learn: tangani kelas tidak seimbang\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "WARNA = {'Negatif': '#FF3B30', 'Netral': '#8E8E93', 'Positif': '#34C759'}\n",
    "LABEL_ORDER = ['Negatif', 'Netral', 'Positif']\n",
    "\n",
    "print('âœ… Semua library berhasil diimport!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Cell 2 â€” Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Prioritas: gunakan dataset_ml.csv (full), atau split jika sudah ada â”€â”€â”€\n",
    "import os\n",
    "\n",
    "USE_PRESPLIT = os.path.exists('dataset_train.csv') and os.path.exists('dataset_test.csv')\n",
    "\n",
    "if USE_PRESPLIT:\n",
    "    df_train_raw = pd.read_csv('dataset_train.csv', encoding='utf-8-sig')\n",
    "    df_test_raw  = pd.read_csv('dataset_test.csv',  encoding='utf-8-sig')\n",
    "    print(f'âœ… Menggunakan pre-split dataset')\n",
    "    print(f'   Train: {len(df_train_raw)} | Test: {len(df_test_raw)}')\n",
    "else:\n",
    "    # Load full dataset dan split manual\n",
    "    for enc in ['utf-8-sig', 'utf-8', 'latin-1']:\n",
    "        try:\n",
    "            df_full = pd.read_csv('dataset_ml.csv', encoding=enc)\n",
    "            print(f'âœ… Loaded dataset_ml.csv ({enc})')\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    USE_PRESPLIT = False\n",
    "\n",
    "# Fungsi normalisasi dataframe\n",
    "def normalize_df(df):\n",
    "    df = df.copy()\n",
    "    df['teks_bersih'] = df['teks_bersih'].fillna('').astype(str).str.strip()\n",
    "    df = df[df['teks_bersih'].str.len() > 0].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "if USE_PRESPLIT:\n",
    "    df_train_raw = normalize_df(df_train_raw)\n",
    "    df_test_raw  = normalize_df(df_test_raw)\n",
    "    df_full = pd.concat([df_train_raw, df_test_raw], ignore_index=True)\n",
    "else:\n",
    "    df_full = normalize_df(df_full)\n",
    "\n",
    "print(f'\\nðŸ“Š Total data: {len(df_full)}')\n",
    "print(f'ðŸ“‹ Kolom    : {list(df_full.columns)}')\n",
    "display(df_full[['teks_bersih', 'label', 'label_id']].head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Cell 3 â€” Eksplorasi & Analisis Distribusi Kelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = df_full['label'].value_counts()\n",
    "total = len(df_full)\n",
    "\n",
    "print('=' * 50)\n",
    "print('ðŸ“Š DISTRIBUSI KELAS')\n",
    "print('=' * 50)\n",
    "for label in LABEL_ORDER:\n",
    "    n = dist.get(label, 0)\n",
    "    pct = n / total * 100\n",
    "    bar = 'â–ˆ' * int(pct / 2.5)\n",
    "    print(f'  {label:10s} {bar:30s} {n:4d} ({pct:.1f}%)')\n",
    "print(f'  Total     {\" \" * 30} {total}')\n",
    "\n",
    "# Hitung imbalance ratio\n",
    "n_max = dist.max()\n",
    "n_min = dist.min()\n",
    "ratio = n_max / n_min\n",
    "print(f'\\nâš ï¸  Imbalance Ratio: {ratio:.1f}x')\n",
    "print(f'   Kelas mayoritas ({dist.idxmax()}): {n_max}')\n",
    "print(f'   Kelas minoritas ({dist.idxmin()}): {n_min}')\n",
    "\n",
    "if ratio > 5:\n",
    "    print('\\nâ— Dataset SANGAT TIDAK SEIMBANG.')\n",
    "    print('   Akan digunakan strategi: class_weight + ComplementNB + oversampling')\n",
    "elif ratio > 2:\n",
    "    print('\\nâš¡ Dataset cukup tidak seimbang, akan digunakan class_weight.')\n",
    "else:\n",
    "    print('\\nâœ… Dataset cukup seimbang.')\n",
    "\n",
    "# Statistik panjang teks per kelas\n",
    "print('\\nâ”€â”€â”€ Rata-rata panjang teks per kelas â”€â”€â”€')\n",
    "df_full['n_kata'] = df_full['teks_bersih'].str.split().str.len()\n",
    "for label in LABEL_ORDER:\n",
    "    sub = df_full[df_full['label'] == label]['n_kata']\n",
    "    if len(sub) > 0:\n",
    "        print(f'  {label:10s}: rata {sub.mean():.1f} kata, max {sub.max()} kata')\n",
    "\n",
    "# Visualisasi\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "fig.patch.set_facecolor('#F2F2F7')\n",
    "\n",
    "# Bar distribusi\n",
    "ax = axes[0]\n",
    "ax.set_facecolor('white')\n",
    "bars = ax.bar(LABEL_ORDER,\n",
    "              [dist.get(l, 0) for l in LABEL_ORDER],\n",
    "              color=[WARNA[l] for l in LABEL_ORDER],\n",
    "              edgecolor='white', width=0.55)\n",
    "for bar, label in zip(bars, LABEL_ORDER):\n",
    "    h = bar.get_height()\n",
    "    pct = h / total * 100\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, h + 2,\n",
    "            f'{int(h)}\\n({pct:.1f}%)', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.set_title('Distribusi Kelas', fontweight='bold', fontsize=13)\n",
    "ax.set_ylabel('Jumlah Sampel')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Boxplot panjang teks\n",
    "ax2 = axes[1]\n",
    "ax2.set_facecolor('white')\n",
    "data_box = [df_full[df_full['label'] == l]['n_kata'].values for l in LABEL_ORDER]\n",
    "bp = ax2.boxplot(data_box, patch_artist=True, labels=LABEL_ORDER,\n",
    "                 medianprops=dict(color='white', linewidth=2))\n",
    "for patch, label in zip(bp['boxes'], LABEL_ORDER):\n",
    "    patch.set_facecolor(WARNA[label])\n",
    "    patch.set_alpha(0.8)\n",
    "ax2.set_title('Sebaran Panjang Teks (kata)', fontweight='bold', fontsize=13)\n",
    "ax2.set_ylabel('Jumlah Kata')\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "plt.savefig('ml_distribusi_kelas.png', dpi=150, bbox_inches='tight', facecolor='#F2F2F7')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Cell 4 â€” Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_full['teks_bersih'].values\n",
    "y = df_full['label'].values\n",
    "\n",
    "if USE_PRESPLIT:\n",
    "    X_train = df_train_raw['teks_bersih'].values\n",
    "    y_train = df_train_raw['label'].values\n",
    "    X_test  = df_test_raw['teks_bersih'].values\n",
    "    y_test  = df_test_raw['label'].values\n",
    "    print('âœ… Menggunakan pre-split (dari file train/test terpisah)')\n",
    "else:\n",
    "    # Stratified split â€” proporsional tiap kelas\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    print('âœ… Stratified split 80/20 selesai')\n",
    "\n",
    "print(f'\\n   Train : {len(X_train)} sampel')\n",
    "print(f'   Test  : {len(X_test)} sampel')\n",
    "\n",
    "# Verifikasi distribusi di train & test\n",
    "print('\\nâ”€â”€â”€ Distribusi label di setiap split â”€â”€â”€')\n",
    "for split_name, y_split in [('Train', y_train), ('Test', y_test)]:\n",
    "    vals, counts = np.unique(y_split, return_counts=True)\n",
    "    d = dict(zip(vals, counts))\n",
    "    print(f'  {split_name}: ', end='')\n",
    "    print(' | '.join([f'{l}={d.get(l,0)}' for l in LABEL_ORDER]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¢ Cell 5 â€” TF-IDF Vectorizer\n",
    "\n",
    "> **TF-IDF** (Term Frequencyâ€“Inverse Document Frequency) mengubah teks menjadi\n",
    "> vektor angka. Kata yang sering muncul di satu dokumen tapi jarang di seluruh\n",
    "> corpus akan mendapat skor tinggi â€” artinya kata itu *deskriptif*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords gabungan Indonesia + Sunda\n",
    "STOPWORDS = [\n",
    "    'yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'ada', 'dengan',\n",
    "    'untuk', 'pada', 'akan', 'adalah', 'juga', 'bisa', 'nya', 'saja',\n",
    "    'sudah', 'agar', 'atau', 'karena', 'jika', 'bila',\n",
    "    'kita', 'kami', 'saya', 'kamu', 'dia', 'mereka', 'anda',\n",
    "    'bapak', 'ibu', 'pak', 'bu', 'pa', 'min', 'kak', 'bang',\n",
    "    # Sunda\n",
    "    'mah', 'teh', 'wae', 'oge', 'eta', 'atuh', 'yeuh', 'euy',\n",
    "    'naha', 'kumaha', 'naon', 'iraha', 'keneh',\n",
    "    # Chat slang\n",
    "    'banget', 'bgt', 'aja', 'ajah', 'dong', 'deh', 'yah', 'ya',\n",
    "    'yg', 'jd', 'tp', 'dgn', 'utk', 'krn', 'pd', 'sm', 'dr',\n",
    "]\n",
    "\n",
    "# Konfigurasi TF-IDF\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),        # unigram + bigram\n",
    "    min_df=2,                  # abaikan kata yang muncul < 2 kali\n",
    "    max_df=0.90,               # abaikan kata yang terlalu umum (>90% dokumen)\n",
    "    max_features=3000,         # maksimal 3000 fitur\n",
    "    sublinear_tf=True,         # gunakan log(TF) agar tidak terlalu dominan\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    stop_words=STOPWORDS\n",
    ")\n",
    "\n",
    "# Fit pada data train, transform train & test\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "\n",
    "print('âœ… TF-IDF Vectorizer selesai')\n",
    "print(f'   Jumlah fitur (vocabulary): {len(tfidf.vocabulary_)}')\n",
    "print(f'   Dimensi matrix train     : {X_train_tfidf.shape}')\n",
    "print(f'   Dimensi matrix test      : {X_test_tfidf.shape}')\n",
    "\n",
    "# Tampilkan top fitur global\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "mean_tfidf = np.asarray(X_train_tfidf.mean(axis=0)).flatten()\n",
    "top_idx = mean_tfidf.argsort()[::-1][:20]\n",
    "\n",
    "print('\\nâ”€â”€â”€ Top 20 fitur TF-IDF (rata-rata tertinggi) â”€â”€â”€')\n",
    "print(' | '.join(feature_names[top_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Cell 6 â€” Training Model Naive Bayes\n",
    "\n",
    "Kita latih **dua varian** Naive Bayes:\n",
    "\n",
    "| Model | Kelebihan |\n",
    "|---|---|\n",
    "| **MultinomialNB** | Standar untuk klasifikasi teks, cepat |\n",
    "| **ComplementNB** | Dirancang khusus untuk data tidak seimbang (*imbalanced*) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung class weight untuk tangani imbalanced data\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, weights))\n",
    "\n",
    "print('âš–ï¸  Class weights (untuk tangani imbalanced data):')\n",
    "for label, w in sorted(class_weight_dict.items()):\n",
    "    print(f'   {label:10s}: {w:.3f}')\n",
    "print()\n",
    "\n",
    "# â”€â”€â”€ Model 1: MultinomialNB â”€â”€â”€\n",
    "mnb = MultinomialNB(alpha=0.5)   # alpha = smoothing Laplace\n",
    "mnb.fit(X_train_tfidf, y_train)\n",
    "y_pred_mnb = mnb.predict(X_test_tfidf)\n",
    "acc_mnb = accuracy_score(y_test, y_pred_mnb)\n",
    "f1_mnb  = f1_score(y_test, y_pred_mnb, average='macro', zero_division=0)\n",
    "\n",
    "# â”€â”€â”€ Model 2: ComplementNB (lebih baik untuk imbalanced) â”€â”€â”€\n",
    "cnb = ComplementNB(alpha=0.5)\n",
    "cnb.fit(X_train_tfidf, y_train)\n",
    "y_pred_cnb = cnb.predict(X_test_tfidf)\n",
    "acc_cnb = accuracy_score(y_test, y_pred_cnb)\n",
    "f1_cnb  = f1_score(y_test, y_pred_cnb, average='macro', zero_division=0)\n",
    "\n",
    "print('ðŸ“ˆ Hasil awal (sebelum tuning):')\n",
    "print(f'   MultinomialNB  â€” Accuracy: {acc_mnb:.4f} | Macro F1: {f1_mnb:.4f}')\n",
    "print(f'   ComplementNB   â€” Accuracy: {acc_cnb:.4f} | Macro F1: {f1_cnb:.4f}')\n",
    "\n",
    "# Pilih model terbaik\n",
    "if f1_cnb >= f1_mnb:\n",
    "    best_model = cnb\n",
    "    best_name  = 'ComplementNB'\n",
    "    y_pred_best = y_pred_cnb\n",
    "else:\n",
    "    best_model = mnb\n",
    "    best_name  = 'MultinomialNB'\n",
    "    y_pred_best = y_pred_mnb\n",
    "\n",
    "print(f'\\nðŸ† Model terpilih: {best_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Cell 7 â€” Cross Validation (5-Fold Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline lengkap: TF-IDF + model\n",
    "pipeline_mnb = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        ngram_range=(1, 2), min_df=2, max_df=0.90,\n",
    "        max_features=3000, sublinear_tf=True,\n",
    "        stop_words=STOPWORDS\n",
    "    )),\n",
    "    ('clf', MultinomialNB(alpha=0.5))\n",
    "])\n",
    "\n",
    "pipeline_cnb = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        ngram_range=(1, 2), min_df=2, max_df=0.90,\n",
    "        max_features=3000, sublinear_tf=True,\n",
    "        stop_words=STOPWORDS\n",
    "    )),\n",
    "    ('clf', ComplementNB(alpha=0.5))\n",
    "])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print('ðŸ”„ Running 5-Fold Stratified Cross Validation...\\n')\n",
    "\n",
    "results = {}\n",
    "for name, pipeline in [('MultinomialNB', pipeline_mnb), ('ComplementNB', pipeline_cnb)]:\n",
    "    scores_acc = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "    scores_f1  = cross_val_score(pipeline, X, y, cv=cv, scoring='f1_macro')\n",
    "    results[name] = {\n",
    "        'acc_mean': scores_acc.mean(),\n",
    "        'acc_std':  scores_acc.std(),\n",
    "        'f1_mean':  scores_f1.mean(),\n",
    "        'f1_std':   scores_f1.std(),\n",
    "        'acc_folds': scores_acc,\n",
    "        'f1_folds':  scores_f1,\n",
    "    }\n",
    "    print(f'  {name}:')\n",
    "    print(f'    Accuracy : {scores_acc.mean():.4f} Â± {scores_acc.std():.4f}')\n",
    "    print(f'    Macro F1 : {scores_f1.mean():.4f} Â± {scores_f1.std():.4f}')\n",
    "    print(f'    Per fold : {[f\"{s:.3f}\" for s in scores_f1]}')\n",
    "    print()\n",
    "\n",
    "# Visualisasi CV scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "fig.patch.set_facecolor('#F2F2F7')\n",
    "\n",
    "for idx, metric in enumerate(['acc_folds', 'f1_folds']):\n",
    "    ax = axes[idx]\n",
    "    ax.set_facecolor('white')\n",
    "    title = 'Accuracy' if idx == 0 else 'Macro F1'\n",
    "    colors_cv = ['#007AFF', '#5856D6']\n",
    "    fold_labels = [f'Fold {i+1}' for i in range(5)]\n",
    "\n",
    "    x_pos = np.arange(5)\n",
    "    w = 0.35\n",
    "    for i, (name, color) in enumerate(zip(results.keys(), colors_cv)):\n",
    "        ax.bar(x_pos + i * w, results[name][metric], w,\n",
    "               label=name, color=color, alpha=0.85, edgecolor='white')\n",
    "\n",
    "    ax.set_xticks(x_pos + w / 2)\n",
    "    ax.set_xticklabels(fold_labels)\n",
    "    ax.set_title(f'CV {title} per Fold', fontweight='bold')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ml_crossval.png', dpi=150, bbox_inches='tight', facecolor='#F2F2F7')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Cell 8 â€” Evaluasi Detail & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print(f'ðŸ“Š CLASSIFICATION REPORT â€” {best_name}')\n",
    "print('=' * 60)\n",
    "print(classification_report(y_test, y_pred_best, target_names=LABEL_ORDER,\n",
    "                             zero_division=0))\n",
    "\n",
    "# Metrik per kelas\n",
    "precision = precision_score(y_test, y_pred_best, labels=LABEL_ORDER,\n",
    "                            average=None, zero_division=0)\n",
    "recall    = recall_score(y_test, y_pred_best, labels=LABEL_ORDER,\n",
    "                         average=None, zero_division=0)\n",
    "f1        = f1_score(y_test, y_pred_best, labels=LABEL_ORDER,\n",
    "                     average=None, zero_division=0)\n",
    "support   = [np.sum(y_test == l) for l in LABEL_ORDER]\n",
    "\n",
    "df_report = pd.DataFrame({\n",
    "    'Kelas':     LABEL_ORDER,\n",
    "    'Precision': [f'{p:.3f}' for p in precision],\n",
    "    'Recall':    [f'{r:.3f}' for r in recall],\n",
    "    'F1-Score':  [f'{f:.3f}' for f in f1],\n",
    "    'Support':   support,\n",
    "})\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred_best, average='macro', zero_division=0)\n",
    "acc      = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "print(f'\\nðŸ“ˆ Ringkasan:')\n",
    "print(f'   Accuracy       : {acc:.4f} ({acc*100:.2f}%)')\n",
    "print(f'   Macro F1       : {macro_f1:.4f}')\n",
    "print(f'   Weighted F1    : {f1_score(y_test, y_pred_best, average=\"weighted\", zero_division=0):.4f}')\n",
    "\n",
    "print()\n",
    "display(df_report)\n",
    "\n",
    "# Visualisasi metrik per kelas\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "fig.patch.set_facecolor('#F2F2F7')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "x = np.arange(len(LABEL_ORDER))\n",
    "w = 0.25\n",
    "metrics_vals = [precision, recall, f1]\n",
    "metric_names = ['Precision', 'Recall', 'F1-Score']\n",
    "m_colors     = ['#007AFF', '#5856D6', '#34C759']\n",
    "\n",
    "for i, (vals, name, color) in enumerate(zip(metrics_vals, metric_names, m_colors)):\n",
    "    bars = ax.bar(x + i * w, vals, w, label=name, color=color, alpha=0.85, edgecolor='white')\n",
    "    for bar, val in zip(bars, vals):\n",
    "        if val > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "                    f'{val:.2f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "ax.set_xticks(x + w)\n",
    "ax.set_xticklabels(LABEL_ORDER, fontsize=11)\n",
    "ax.set_ylim(0, 1.15)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title(f'Precision / Recall / F1 per Kelas â€” {best_name}',\n",
    "             fontweight='bold', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.axhline(y=macro_f1, color='gray', linestyle='--', linewidth=1, alpha=0.6,\n",
    "           label=f'Macro F1 = {macro_f1:.3f}')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ml_metrik_kelas.png', dpi=150, bbox_inches='tight', facecolor='#F2F2F7')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ¦ Cell 9 â€” Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.patch.set_facecolor('#F2F2F7')\n",
    "\n",
    "for ax, y_pred, model_name in zip(\n",
    "    axes,\n",
    "    [y_pred_mnb, y_pred_cnb],\n",
    "    ['MultinomialNB', 'ComplementNB']\n",
    "):\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=LABEL_ORDER)\n",
    "    cm_pct = cm.astype(float) / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "    # Buat label dengan angka + persentase\n",
    "    annot = np.array(\n",
    "        [[f'{cm[i,j]}\\n({cm_pct[i,j]:.0f}%)' for j in range(len(LABEL_ORDER))]\n",
    "         for i in range(len(LABEL_ORDER))]\n",
    "    )\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm_pct, annot=annot, fmt='',\n",
    "        xticklabels=LABEL_ORDER, yticklabels=LABEL_ORDER,\n",
    "        cmap='Blues', ax=ax,\n",
    "        linewidths=0.5, linecolor='#F2F2F7',\n",
    "        cbar_kws={'shrink': 0.8}\n",
    "    )\n",
    "    acc_m = accuracy_score(y_test, y_pred)\n",
    "    ax.set_title(f'{model_name}\\nAccuracy: {acc_m:.4f}', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('Prediksi', fontsize=11)\n",
    "    ax.set_ylabel('Aktual', fontsize=11)\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "plt.suptitle('Confusion Matrix (% dari aktual)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('ml_confusion_matrix.png', dpi=150, bbox_inches='tight', facecolor='#F2F2F7')\n",
    "plt.show()\n",
    "\n",
    "# Interpretasi diagonal\n",
    "print('\\nðŸ“ Interpretasi Confusion Matrix:')\n",
    "cm_best = confusion_matrix(y_test, y_pred_best, labels=LABEL_ORDER)\n",
    "for i, label in enumerate(LABEL_ORDER):\n",
    "    row_sum = cm_best[i].sum()\n",
    "    correct = cm_best[i, i]\n",
    "    if row_sum > 0:\n",
    "        print(f'   {label:10s}: {correct}/{row_sum} benar ({correct/row_sum*100:.1f}% recall)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¤ Cell 10 â€” Fitur Paling Informatif per Kelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil log probability tiap fitur per kelas dari ComplementNB\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "N_TOP = 15\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.patch.set_facecolor('#F2F2F7')\n",
    "\n",
    "# Dapatkan urutan kelas dari model\n",
    "model_classes = list(best_model.classes_)\n",
    "\n",
    "for ax, label in zip(axes, LABEL_ORDER):\n",
    "    if label not in model_classes:\n",
    "        ax.text(0.5, 0.5, f'Kelas {label}\\ntidak ada di train',\n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "\n",
    "    idx_class = model_classes.index(label)\n",
    "    log_probs = best_model.feature_log_prob_[idx_class]\n",
    "\n",
    "    # Top N fitur dengan probabilitas tertinggi\n",
    "    top_idx = log_probs.argsort()[::-1][:N_TOP]\n",
    "    top_features = feature_names[top_idx]\n",
    "    top_scores   = np.exp(log_probs[top_idx])  # konversi ke probabilitas\n",
    "\n",
    "    color = WARNA[label]\n",
    "    ax.set_facecolor('white')\n",
    "    ax.barh(list(reversed(top_features)), list(reversed(top_scores)),\n",
    "            color=color, alpha=0.85, edgecolor='white')\n",
    "    ax.set_title(f'Top Fitur â€” {label}', fontweight='bold', fontsize=12, color=color)\n",
    "    ax.set_xlabel('P(kata | kelas)', fontsize=9)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.suptitle(f'Fitur Paling Informatif per Kelas â€” {best_name}',\n",
    "             fontsize=13, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('ml_top_fitur.png', dpi=150, bbox_inches='tight', facecolor='#F2F2F7')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âŒ Cell 11 â€” Analisis Kesalahan Prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat dataframe hasil prediksi\n",
    "df_hasil = pd.DataFrame({\n",
    "    'teks':          X_test,\n",
    "    'label_aktual':  y_test,\n",
    "    'pred_mnb':      y_pred_mnb,\n",
    "    'pred_cnb':      y_pred_cnb,\n",
    "    'pred_best':     y_pred_best,\n",
    "})\n",
    "\n",
    "# Probabilitas prediksi (confidence)\n",
    "prob_best = best_model.predict_proba(X_test_tfidf)\n",
    "df_hasil['confidence'] = prob_best.max(axis=1)\n",
    "\n",
    "# Tandai kesalahan\n",
    "df_hasil['benar'] = df_hasil['label_aktual'] == df_hasil['pred_best']\n",
    "df_salah = df_hasil[~df_hasil['benar']].copy()\n",
    "\n",
    "print(f'âœ… Prediksi benar : {df_hasil[\"benar\"].sum()} / {len(df_hasil)}')\n",
    "print(f'âŒ Prediksi salah : {(~df_hasil[\"benar\"]).sum()} / {len(df_hasil)}')\n",
    "\n",
    "if len(df_salah) > 0:\n",
    "    print(f'\\nâ”€â”€â”€ Pola Kesalahan â”€â”€â”€')\n",
    "    pola = df_salah.groupby(['label_aktual', 'pred_best']).size().reset_index(name='count')\n",
    "    pola = pola.sort_values('count', ascending=False)\n",
    "    for _, row in pola.iterrows():\n",
    "        print(f'   {row[\"label_aktual\"]:10s} â†’ diprediksi {row[\"pred_best\"]:10s}: {row[\"count\"]} kasus')\n",
    "\n",
    "    print(f'\\nâ”€â”€â”€ 10 Kesalahan dengan Confidence Tertinggi â”€â”€â”€')\n",
    "    df_salah_sorted = df_salah.sort_values('confidence', ascending=False).head(10)\n",
    "    for _, row in df_salah_sorted.iterrows():\n",
    "        teks_preview = row['teks'][:65] + '...' if len(row['teks']) > 65 else row['teks']\n",
    "        print(f'   Aktual: {row[\"label_aktual\"]:10s} | Prediksi: {row[\"pred_best\"]:10s} | '\n",
    "              f'Conf: {row[\"confidence\"]:.3f}')\n",
    "        print(f'   Teks  : \"{teks_preview}\"')\n",
    "        print()\n",
    "\n",
    "# Export hasil prediksi lengkap\n",
    "df_hasil.to_csv('hasil_prediksi_test.csv', index=False, encoding='utf-8-sig')\n",
    "print('ðŸ’¾ Tersimpan: hasil_prediksi_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Cell 12 â€” Prediksi Teks Baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str): return ''\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'-]', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def prediksi(teks_input, verbose=True):\n",
    "    \"\"\"\n",
    "    Prediksi sentimen teks baru menggunakan model terlatih.\n",
    "    Returns: dict dengan label, confidence, dan probabilitas tiap kelas\n",
    "    \"\"\"\n",
    "    bersih = preprocess(teks_input)\n",
    "    if not bersih:\n",
    "        return {'label': 'Netral', 'confidence': 0.0, 'teks_bersih': ''}\n",
    "\n",
    "    vec = tfidf.transform([bersih])\n",
    "    label = best_model.predict(vec)[0]\n",
    "    probs = best_model.predict_proba(vec)[0]\n",
    "    prob_dict = dict(zip(best_model.classes_, probs))\n",
    "    confidence = probs.max()\n",
    "\n",
    "    if verbose:\n",
    "        icon = {'Negatif': 'ðŸ”´', 'Netral': 'âšª', 'Positif': 'ðŸŸ¢'}.get(label, 'âš«')\n",
    "        print(f'  Input     : \"{teks_input[:70]}\"')\n",
    "        print(f'  Bersih    : \"{bersih[:70]}\"')\n",
    "        print(f'  Prediksi  : {icon} {label}  (confidence: {confidence:.3f})')\n",
    "        for l in LABEL_ORDER:\n",
    "            p = prob_dict.get(l, 0)\n",
    "            bar = 'â–“' * int(p * 20)\n",
    "            print(f'  {l:10s}: {bar:20s} {p:.3f}')\n",
    "        print()\n",
    "\n",
    "    return {'label': label, 'confidence': confidence,\n",
    "            'probabilitas': prob_dict, 'teks_bersih': bersih}\n",
    "\n",
    "\n",
    "# â”€â”€â”€ Uji coba prediksi â”€â”€â”€\n",
    "teks_uji = [\n",
    "    'Alhamdulillah jalan depan rumah akhirnya diperbaiki, terima kasih pak bupati!',\n",
    "    'Jalan masih rusak parah, kapan diperbaiki? Sudah bertahun-tahun begini.',\n",
    "    'Cukup bagus kepemimpinan bapa bupati, semoga terus maju.',\n",
    "    'Ga ngaruh apa2. Sama aja bohong. Hahaha.',\n",
    "    'Hmm biasa aja sih, gak ada yang spesial.',\n",
    "    'Pengangguran masih tinggi, lapangan kerja kurang, galian pasir masih beroperasi.',\n",
    "]\n",
    "\n",
    "print('=' * 60)\n",
    "print('ðŸ§ª PREDIKSI TEKS BARU')\n",
    "print('=' * 60)\n",
    "for teks in teks_uji:\n",
    "    prediksi(teks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Cell 13 â€” Simpan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Simpan model & vectorizer\n",
    "joblib.dump(best_model, 'model_naive_bayes.pkl')\n",
    "joblib.dump(tfidf, 'vectorizer_tfidf.pkl')\n",
    "print(f'âœ… Model tersimpan  : model_naive_bayes.pkl')\n",
    "print(f'âœ… Vectorizer simpan: vectorizer_tfidf.pkl')\n",
    "\n",
    "# Simpan metadata model\n",
    "model_info = {\n",
    "    'nama_model':    best_name,\n",
    "    'vectorizer':    'TF-IDF',\n",
    "    'tfidf_params': {\n",
    "        'ngram_range': '(1, 2)',\n",
    "        'min_df': 2,\n",
    "        'max_df': 0.90,\n",
    "        'max_features': 3000,\n",
    "        'sublinear_tf': True,\n",
    "    },\n",
    "    'hasil_evaluasi': {\n",
    "        'accuracy':   round(float(accuracy_score(y_test, y_pred_best)), 4),\n",
    "        'macro_f1':   round(float(f1_score(y_test, y_pred_best, average='macro', zero_division=0)), 4),\n",
    "        'per_kelas': {\n",
    "            label: {\n",
    "                'precision': round(float(precision_score(y_test, y_pred_best, labels=[label], average='macro', zero_division=0)), 4),\n",
    "                'recall':    round(float(recall_score(y_test, y_pred_best, labels=[label], average='macro', zero_division=0)), 4),\n",
    "                'f1':        round(float(f1_score(y_test, y_pred_best, labels=[label], average='macro', zero_division=0)), 4),\n",
    "                'support':   int(np.sum(y_test == label)),\n",
    "            } for label in LABEL_ORDER\n",
    "        },\n",
    "    },\n",
    "    'cross_validation': {\n",
    "        model: {\n",
    "            'accuracy_mean': round(results[model]['acc_mean'], 4),\n",
    "            'f1_mean':       round(results[model]['f1_mean'], 4),\n",
    "        } for model in results\n",
    "    },\n",
    "    'label_map':      {'Negatif': 0, 'Netral': 1, 'Positif': 2},\n",
    "    'cara_pakai': [\n",
    "        \"import joblib\",\n",
    "        \"model = joblib.load('model_naive_bayes.pkl')\",\n",
    "        \"tfidf = joblib.load('vectorizer_tfidf.pkl')\",\n",
    "        \"teks_bersih = preprocess(teks_baru)  # gunakan fungsi preprocess()\",\n",
    "        \"X = tfidf.transform([teks_bersih])\",\n",
    "        \"label = model.predict(X)[0]\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('model_info.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_info, f, ensure_ascii=False, indent=2)\n",
    "print('âœ… Tersimpan: model_info.json')\n",
    "\n",
    "# ZIP semua output\n",
    "output_files = [\n",
    "    'model_naive_bayes.pkl', 'vectorizer_tfidf.pkl', 'model_info.json',\n",
    "    'hasil_prediksi_test.csv',\n",
    "    'ml_distribusi_kelas.png', 'ml_crossval.png',\n",
    "    'ml_metrik_kelas.png', 'ml_confusion_matrix.png', 'ml_top_fitur.png',\n",
    "]\n",
    "\n",
    "with zipfile.ZipFile('SENTALIS_ML_output.zip', 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    for fname in output_files:\n",
    "        if os.path.exists(fname):\n",
    "            zf.write(fname)\n",
    "            print(f'  + {fname}')\n",
    "\n",
    "print(f'\\nðŸ“¦ ZIP: SENTALIS_ML_output.zip ({os.path.getsize(\"SENTALIS_ML_output.zip\")/1024:.1f} KB)')\n",
    "\n",
    "# Auto-download di Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('SENTALIS_ML_output.zip')\n",
    "    print('â¬‡ï¸  Download dimulai...')\n",
    "except ImportError:\n",
    "    print('ðŸ“‚ File tersimpan di direktori saat ini.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”® Cell 14 â€” Load Model & Prediksi Ulang (Opsional)\n",
    "\n",
    "> Cell ini untuk membuktikan bahwa model yang disimpan bisa diload dan digunakan\n",
    "> tanpa perlu training ulang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model dari file\n",
    "model_loaded  = joblib.load('model_naive_bayes.pkl')\n",
    "tfidf_loaded  = joblib.load('vectorizer_tfidf.pkl')\n",
    "\n",
    "print('âœ… Model & vectorizer berhasil di-load dari file')\n",
    "print(f'   Tipe model: {type(model_loaded).__name__}')\n",
    "print(f'   Kelas model: {model_loaded.classes_}')\n",
    "print()\n",
    "\n",
    "# Coba prediksi menggunakan model yang di-load\n",
    "def prediksi_dari_file(teks):\n",
    "    bersih = preprocess(teks)\n",
    "    vec = tfidf_loaded.transform([bersih])\n",
    "    label = model_loaded.predict(vec)[0]\n",
    "    conf  = model_loaded.predict_proba(vec)[0].max()\n",
    "    icon  = {'Negatif': 'ðŸ”´', 'Netral': 'âšª', 'Positif': 'ðŸŸ¢'}.get(label, 'âš«')\n",
    "    print(f'  {icon} {label} ({conf:.3f}) | \"{teks[:60]}\"')\n",
    "\n",
    "print('ðŸ”® Test prediksi dari model yang di-load:')\n",
    "prediksi_dari_file('Jalan rusak parah, sudah lama tidak diperbaiki!')\n",
    "prediksi_dari_file('Terima kasih, programnya sangat bermanfaat untuk warga.')\n",
    "prediksi_dari_file('Biasa aja, tidak ada yang berubah.')\n",
    "\n",
    "print('\\nâœ… Model siap digunakan!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š Ringkasan & Catatan Penting\n",
    "\n",
    "### Tentang Imbalanced Data\n",
    "\n",
    "Dataset ini memiliki distribusi yang sangat tidak seimbang:\n",
    "- **Netral ~79%** â€” kelas mayoritas\n",
    "- **Negatif ~17%** â€” kelas sedang  \n",
    "- **Positif ~4%** â€” kelas minoritas âš ï¸\n",
    "\n",
    "Akibatnya, **accuracy tinggi bisa menyesatkan** â€” model bisa saja selalu memprediksi \"Netral\" dan tetap dapat accuracy >79%. Gunakan **Macro F1** sebagai metrik utama.\n",
    "\n",
    "### Cara Tingkatkan Performa\n",
    "\n",
    "| Strategi | Cara |\n",
    "|---|---|\n",
    "| **Tambah data Positif** | Cari/tambahkan lebih banyak komentar positif berlabel |\n",
    "| **Human annotation** | Review manual terutama komentar Positif |\n",
    "| **Ganti model** | Coba IndoBERT untuk pemahaman konteks lebih baik |\n",
    "| **Tuning alpha** | Eksperimen nilai alpha 0.1 â€“ 1.0 pada Naive Bayes |\n",
    "\n",
    "### Cara Pakai Model\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "model = joblib.load('model_naive_bayes.pkl')\n",
    "tfidf = joblib.load('vectorizer_tfidf.pkl')\n",
    "\n",
    "teks = \"Jalan masih rusak dan gelap!\"\n",
    "X = tfidf.transform([teks.lower()])\n",
    "label = model.predict(X)[0]      # 'Negatif'\n",
    "probs = model.predict_proba(X)   # array probabilitas\n",
    "```\n",
    "\n",
    "---\n",
    "*SENTALIS â€” Machine Learning Edition*  \n",
    "*TF-IDF + Naive Bayes (Multinomial & Complement)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "name": "SENTALIS_TFIDF_NaiveBayes.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
