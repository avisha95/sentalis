{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  SENTALIS â€” Rule-Based Sentiment Analysis\n",
    "## + Export Dataset untuk Machine Learning\n",
    "\n",
    "**Notebook ini mencakup:**\n",
    "1. Load & preview data komentar Instagram\n",
    "2. Text preprocessing (cleaning, normalisasi)\n",
    "3. Rule-based sentiment classification (Indonesia + Sunda)\n",
    "4. Visualisasi hasil analisis\n",
    "5. **Export dataset berlabel siap pakai untuk training ML**\n",
    "\n",
    "---\n",
    "> ğŸ“ Letakkan file `komentar_instagram.csv` di direktori yang sama dengan notebook ini,\n",
    "> atau upload ke Google Colab terlebih dahulu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Cell 1 â€” Install & Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install library yang dibutuhkan\n",
    "!pip install pandas matplotlib seaborn wordcloud openpyxl -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "# Agar grafik tampil di notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "print('âœ… Semua library berhasil diimport!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‚ Cell 2 â€” Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Ganti path jika file ada di lokasi lain â”€â”€â”€\n",
    "CSV_PATH = 'komentar_instagram.csv'\n",
    "\n",
    "# Auto-detect encoding\n",
    "for enc in ['utf-8-sig', 'utf-8', 'latin-1']:\n",
    "    try:\n",
    "        df = pd.read_csv(CSV_PATH, encoding=enc)\n",
    "        print(f'âœ… File berhasil dibaca dengan encoding: {enc}')\n",
    "        break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Normalisasi nama kolom\n",
    "df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]\n",
    "\n",
    "# Pastikan kolom 'komentar' ada\n",
    "if 'komentar' not in df.columns:\n",
    "    raise ValueError(f'Kolom \"komentar\" tidak ditemukan. Kolom yang ada: {list(df.columns)}')\n",
    "\n",
    "# Hapus baris tanpa teks komentar\n",
    "df = df.dropna(subset=['komentar']).reset_index(drop=True)\n",
    "df['komentar'] = df['komentar'].astype(str).str.strip()\n",
    "df = df[df['komentar'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(f'\\nğŸ“Š Total data: {len(df)} baris')\n",
    "print(f'ğŸ“‹ Kolom: {list(df.columns)}')\n",
    "print(f'\\n--- Preview 5 baris pertama ---')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Cell 3 â€” Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Membersihkan dan menormalisasi teks komentar.\n",
    "    - Lowercase\n",
    "    - Hapus URL, mention, hashtag\n",
    "    - Hapus karakter non-ASCII yang tidak perlu\n",
    "    - Normalisasi spasi berlebih\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)          # hapus URL\n",
    "    text = re.sub(r'@\\w+', '', text)                        # hapus mention\n",
    "    text = re.sub(r'#\\w+', '', text)                        # hapus hashtag\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'-]', ' ', text)            # hapus karakter aneh\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)             # kurangi repetisi (hahaha -> haha)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()               # normalisasi spasi\n",
    "    return text\n",
    "\n",
    "df['teks_bersih'] = df['komentar'].apply(preprocess)\n",
    "\n",
    "# Tampilkan perbandingan sebelum & sesudah\n",
    "contoh = df[['komentar', 'teks_bersih']].head(8)\n",
    "print('ğŸ” Contoh hasil preprocessing:\\n')\n",
    "for _, row in contoh.iterrows():\n",
    "    print(f'  Asli  : {row[\"komentar\"][:80]}')\n",
    "    print(f'  Bersih: {row[\"teks_bersih\"][:80]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—‚ï¸ Cell 4 â€” Kamus Kata & Frasa Sentimen\n",
    "\n",
    "> Kamus ini bisa diedit sesuai kebutuhan. Semakin lengkap kamus, semakin akurat hasilnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  KAMUS FRASA NEGATIF\n",
    "#  (dicek lebih dulu untuk menangkap sarkasme & frasa khusus)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "FRASA_NEGATIF = [\n",
    "    # Sarkasme eksplisit\n",
    "    'puas mreun', 'alhamdulillah katanya', 'katanya sudah', 'sok bagus',\n",
    "    'wkwk', 'wkwkwk', 'haha', 'hahaha', 'bisa ketawa',\n",
    "    'alhamdulillah masih', 'alhamdulillah belum', 'alhamdulillah ga',\n",
    "    'alhamdulillah tdk', 'alhamdulillah tidak', 'alhamdulillah blm',\n",
    "    # Kritik infrastruktur\n",
    "    'jalan rusak', 'jalan berlubang', 'jalan parah', 'jalan ancur',\n",
    "    'jalan hancur', 'jalan mana', 'jalan boro', 'jalan barutut',\n",
    "    'masih rusak', 'masih berlubang', 'masih gelap', 'masih banjir',\n",
    "    'masih macet', 'masih kumuh', 'masih kotor',\n",
    "    # Kritik pemerintah\n",
    "    'tidak ada perubahan', 'ga ada perubahan', 'gak ada perubahan',\n",
    "    'teu ngaruh', 'teu aya', 'teu robah', 'teu berubah',\n",
    "    'ga ngaruh', 'gak ngaruh', 'tidak ngaruh',\n",
    "    'ga ada', 'gak ada', 'tidak ada hasil',\n",
    "    'kitu keneh', 'kitu wae', 'kitu deui', 'kitu bae',\n",
    "    'php', 'gimmick', 'pencitraan', 'omong kosong', 'janji palsu',\n",
    "    'bohong', 'tidak terbukti', 'ga terbukti', 'gak terbukti',\n",
    "    'tidak nyata', 'tidak berhasil', 'ga berhasil',\n",
    "    # Ungkapan kekecewaan\n",
    "    'kecewa', 'mengecewakan', 'nyebelin', 'menyebalkan', 'parah',\n",
    "    'gila', 'kurang ajar', 'carut marut', 'ancur', 'hancur lebur',\n",
    "    'amburadul', 'berantakan', 'tidak beres', 'ga beres',\n",
    "    'kapan selesai', 'kapan beres', 'kapan beneran',\n",
    "    # Sunda negatif\n",
    "    'butut', 'paroek', 'awon', 'goreng pisan', 'teu bener',\n",
    "    'teu puguh', 'teu jelas', 'teu karuhan', 'teu nanaon',\n",
    "    'ngan omongan', 'ukur omongan', 'omong wungkul',\n",
    "    # Keluhan spesifik Sumedang\n",
    "    'galian', 'galian pasir', 'tambang', 'debu', 'bising',\n",
    "    'banjir terus', 'macet terus', 'gelap terus', 'kumuh terus',\n",
    "    'pengangguran', 'susah kerja', 'sulitnya kerja', 'tidak ada kerjaan',\n",
    "    'ga ada kerjaan', 'lapangan kerja kurang',\n",
    "]\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  KATA NEGATIF (kata tunggal)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "KATA_NEGATIF = [\n",
    "    'jelek', 'buruk', 'rusak', 'gagal', 'lambat', 'lambat', 'lemot',\n",
    "    'kotor', 'bau', 'kumuh', 'jorok', 'semrawut', 'kacau', 'amburadul',\n",
    "    'tidak', 'belum', 'kurang', 'minim', 'nihil', 'nol',\n",
    "    'susah', 'sulit', 'payah', 'repot', 'ribet', 'pelik',\n",
    "    'benci', 'marah', 'kesel', 'kesal', 'sebal', 'gondok',\n",
    "    'bohong', 'dusta', 'tipu', 'menipu', 'nipu',\n",
    "    'korupsi', 'korup', 'pungli', 'sogok', 'suap',\n",
    "    'diskriminasi', 'nepotisme', 'kolusi',\n",
    "    'banjir', 'longsor', 'macet', 'sempit', 'gelap',\n",
    "    'mahal', 'mencekik', 'mencekam', 'memberatkan',\n",
    "    # Sunda\n",
    "    'butut', 'awon', 'gorengan', 'teu', 'euweuh',\n",
    "]\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  FRASA POSITIF\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "FRASA_POSITIF = [\n",
    "    'terima kasih', 'terimakasih', 'makasih', 'thank you',\n",
    "    'alhamdulillah ada perubahan', 'ada kemajuan', 'ada perbaikan',\n",
    "    'sudah lebih baik', 'sudah bagus', 'sudah maju', 'semakin bagus',\n",
    "    'semakin baik', 'semakin maju', 'semakin berkembang',\n",
    "    'sukses selalu', 'semoga sukses', 'semoga berhasil',\n",
    "    'sehat selalu', 'semoga sehat', 'lancar terus',\n",
    "    'terus berjuang', 'terus maju', 'terus berkembang',\n",
    "    'kerja bagus', 'kerja keras', 'kerja nyata', 'kerja baik',\n",
    "    'mantap jiwa', 'top banget', 'luar biasa', 'keren banget',\n",
    "    'hebat banget', 'salut', 'salut pisan',\n",
    "    # Sunda positif\n",
    "    'alhamdulillah aya', 'alus pisan', 'sae pisan', 'sae teuing',\n",
    "    'hatur nuhun', 'nuhun', 'sukur',\n",
    "]\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  KATA POSITIF (kata tunggal)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "KATA_POSITIF = [\n",
    "    'bagus', 'baik', 'mantap', 'keren', 'hebat', 'luar biasa',\n",
    "    'sukses', 'berhasil', 'maju', 'berkembang', 'meningkat',\n",
    "    'bersih', 'rapi', 'indah', 'nyaman', 'aman', 'sejuk',\n",
    "    'ramah', 'sopan', 'profesional', 'cepat', 'tepat',\n",
    "    'transparan', 'jujur', 'amanah', 'adil',\n",
    "    'inovatif', 'kreatif', 'solutif', 'responsif',\n",
    "    'senang', 'bahagia', 'puas', 'bangga', 'lega', 'syukur',\n",
    "    'terima', 'apresiasi', 'dukung', 'mendukung',\n",
    "    # Sunda\n",
    "    'alus', 'sae', 'saÃ©',\n",
    "]\n",
    "\n",
    "# Urutkan dari terpanjang ke terpendek (penting untuk matching akurat)\n",
    "FRASA_NEGATIF.sort(key=len, reverse=True)\n",
    "FRASA_POSITIF.sort(key=len, reverse=True)\n",
    "\n",
    "print(f'ğŸ“– Kamus siap:')\n",
    "print(f'   Frasa negatif : {len(FRASA_NEGATIF)} entri')\n",
    "print(f'   Kata negatif  : {len(KATA_NEGATIF)} entri')\n",
    "print(f'   Frasa positif : {len(FRASA_POSITIF)} entri')\n",
    "print(f'   Kata positif  : {len(KATA_POSITIF)} entri')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Cell 5 â€” Fungsi Klasifikasi Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klasifikasi_sentimen(teks):\n",
    "    \"\"\"\n",
    "    Mengklasifikasikan sentimen teks menggunakan pendekatan rule-based.\n",
    "    \n",
    "    Urutan pemeriksaan (hierarki):\n",
    "    1. Frasa negatif (untuk tangkap sarkasme)\n",
    "    2. Frasa positif\n",
    "    3. Kata negatif\n",
    "    4. Kata positif\n",
    "    5. Default: Netral\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (label: str, kata_kunci: list, skor: int)\n",
    "    \"\"\"\n",
    "    if not teks or not isinstance(teks, str) or len(teks.strip()) < 2:\n",
    "        return 'Netral', [], 0\n",
    "    \n",
    "    t = teks.lower()\n",
    "    cocok_negatif = []\n",
    "    cocok_positif = []\n",
    "    \n",
    "    # 1. Cek frasa negatif\n",
    "    for frasa in FRASA_NEGATIF:\n",
    "        if frasa in t:\n",
    "            cocok_negatif.append(frasa)\n",
    "    \n",
    "    # 2. Cek frasa positif\n",
    "    for frasa in FRASA_POSITIF:\n",
    "        if frasa in t:\n",
    "            cocok_positif.append(frasa)\n",
    "    \n",
    "    # 3. Cek kata negatif (hanya jika belum ada frasa negatif)\n",
    "    if not cocok_negatif:\n",
    "        for kata in KATA_NEGATIF:\n",
    "            if re.search(r'\\b' + re.escape(kata) + r'\\b', t):\n",
    "                cocok_negatif.append(kata)\n",
    "    \n",
    "    # 4. Cek kata positif (hanya jika belum ada frasa positif)\n",
    "    if not cocok_positif:\n",
    "        for kata in KATA_POSITIF:\n",
    "            if re.search(r'\\b' + re.escape(kata) + r'\\b', t):\n",
    "                cocok_positif.append(kata)\n",
    "    \n",
    "    # 5. Tentukan label berdasarkan jumlah match\n",
    "    skor_neg = len(cocok_negatif)\n",
    "    skor_pos = len(cocok_positif)\n",
    "    skor_net = skor_pos - skor_neg  # positif = nilai tinggi\n",
    "    \n",
    "    if skor_neg > skor_pos:\n",
    "        return 'Negatif', cocok_negatif[:5], skor_net\n",
    "    elif skor_pos > skor_neg:\n",
    "        return 'Positif', cocok_positif[:5], skor_net\n",
    "    elif skor_neg > 0 and skor_neg == skor_pos:\n",
    "        # Jika sama banyak, frasa negatif lebih prioritas\n",
    "        return 'Negatif', cocok_negatif[:5], skor_net\n",
    "    else:\n",
    "        return 'Netral', [], 0\n",
    "\n",
    "\n",
    "# â”€â”€ Test fungsi â”€â”€\n",
    "test_cases = [\n",
    "    'Ga ngaruh apa2. hahaha',\n",
    "    'Jalan barutut jeng paroek, kapan diperbaiki?',\n",
    "    'Alhamdulillah ada perubahan terutama di fasilitas umum',\n",
    "    'Sukses selalu bapa bupati!',\n",
    "    'Hmmmm',\n",
    "    'Survey na wak',\n",
    "    'Gimmick doang, PHP warga!',\n",
    "]\n",
    "\n",
    "print('ğŸ§ª Test klasifikasi:\\n')\n",
    "for t in test_cases:\n",
    "    bersih = preprocess(t)\n",
    "    label, kunci, skor = klasifikasi_sentimen(bersih)\n",
    "    print(f'  [{label:8s}] skor={skor:+2d} | \"{t[:55]}\"')\n",
    "    if kunci:\n",
    "        print(f'           kata kunci: {kunci}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Cell 6 â€” Jalankan Klasifikasi ke Seluruh Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terapkan klasifikasi ke semua baris\n",
    "hasil = df['teks_bersih'].apply(klasifikasi_sentimen)\n",
    "\n",
    "df['sentimen']    = hasil.apply(lambda x: x[0])   # label sentimen\n",
    "df['kata_kunci']  = hasil.apply(lambda x: ', '.join(x[1]) if x[1] else '')  # kata yang cocok\n",
    "df['skor']        = hasil.apply(lambda x: x[2])   # skor numerik\n",
    "\n",
    "# Ringkasan distribusi\n",
    "dist = df['sentimen'].value_counts()\n",
    "total = len(df)\n",
    "\n",
    "print('ğŸ“Š Hasil Klasifikasi:\\n')\n",
    "for label, count in dist.items():\n",
    "    pct = count / total * 100\n",
    "    bar = 'â–ˆ' * int(pct / 3)\n",
    "    print(f'  {label:10s} {bar:20s} {count:4d} ({pct:.1f}%)')\n",
    "\n",
    "print(f'\\n  Total: {total} komentar')\n",
    "print('\\nâ”€â”€â”€ Preview 10 hasil â”€â”€â”€')\n",
    "display(df[['komentar', 'sentimen', 'skor', 'kata_kunci']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Cell 7 â€” Visualisasi Distribusi Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARNA = {'Negatif': '#FF3B30', 'Netral': '#8E8E93', 'Positif': '#34C759'}\n",
    "\n",
    "dist_sorted = df['sentimen'].value_counts()\n",
    "labels = dist_sorted.index.tolist()\n",
    "counts = dist_sorted.values.tolist()\n",
    "colors = [WARNA.get(l, '#007AFF') for l in labels]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "fig.patch.set_facecolor('#F2F2F7')\n",
    "\n",
    "# â”€â”€ Pie chart â”€â”€\n",
    "ax1 = axes[0]\n",
    "ax1.set_facecolor('#F2F2F7')\n",
    "wedges, texts, autotexts = ax1.pie(\n",
    "    counts, labels=labels, colors=colors,\n",
    "    autopct='%1.1f%%', startangle=140,\n",
    "    wedgeprops=dict(edgecolor='white', linewidth=2.5),\n",
    "    pctdistance=0.75\n",
    ")\n",
    "for at in autotexts:\n",
    "    at.set_fontsize(11)\n",
    "    at.set_fontweight('bold')\n",
    "    at.set_color('white')\n",
    "for t in texts:\n",
    "    t.set_fontsize(12)\n",
    "ax1.set_title('Distribusi Sentimen', fontsize=14, fontweight='bold', pad=15, color='#1C1C1E')\n",
    "\n",
    "# â”€â”€ Bar chart â”€â”€\n",
    "ax2 = axes[1]\n",
    "ax2.set_facecolor('white')\n",
    "bars = ax2.barh(labels, counts, color=colors, edgecolor='white', height=0.55)\n",
    "ax2.set_xlabel('Jumlah Komentar', fontsize=11, color='#3A3A3C')\n",
    "ax2.set_title('Jumlah per Kategori', fontsize=14, fontweight='bold', color='#1C1C1E')\n",
    "ax2.tick_params(colors='#3A3A3C')\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax2.text(count + 1, bar.get_y() + bar.get_height() / 2,\n",
    "             f'{count}', va='center', fontsize=11, fontweight='bold', color='#1C1C1E')\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "plt.savefig('viz_distribusi.png', dpi=150, bbox_inches='tight', facecolor='#F2F2F7')\n",
    "plt.show()\n",
    "print('ğŸ’¾ Tersimpan: viz_distribusi.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Cell 8 â€” Visualisasi per Tipe (Komentar vs Reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tipe' in df.columns:\n",
    "    cross = pd.crosstab(df['tipe'], df['sentimen'])\n",
    "    for col in ['Negatif', 'Netral', 'Positif']:\n",
    "        if col not in cross.columns:\n",
    "            cross[col] = 0\n",
    "    cross = cross[['Negatif', 'Netral', 'Positif']]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    fig.patch.set_facecolor('#F2F2F7')\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    x = np.arange(len(cross.index))\n",
    "    w = 0.25\n",
    "    for i, (col, color) in enumerate(WARNA.items()):\n",
    "        if col in cross.columns:\n",
    "            bars = ax.bar(x + i * w, cross[col], w, label=col, color=color, edgecolor='white')\n",
    "            for bar in bars:\n",
    "                h = bar.get_height()\n",
    "                if h > 0:\n",
    "                    ax.text(bar.get_x() + bar.get_width() / 2, h + 0.5,\n",
    "                            str(int(h)), ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "    ax.set_xticks(x + w)\n",
    "    ax.set_xticklabels(cross.index, fontsize=11)\n",
    "    ax.set_ylabel('Jumlah', fontsize=11)\n",
    "    ax.set_title('Sentimen berdasarkan Tipe Komentar', fontsize=13, fontweight='bold', color='#1C1C1E')\n",
    "    ax.legend(title='Sentimen', fontsize=10)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('viz_tipe.png', dpi=150, bbox_inches='tight', facecolor='#F2F2F7')\n",
    "    plt.show()\n",
    "    print('ğŸ’¾ Tersimpan: viz_tipe.png')\n",
    "else:\n",
    "    print('â„¹ï¸ Kolom \"tipe\" tidak ditemukan, visualisasi ini dilewati.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â˜ï¸ Cell 9 â€” WordCloud per Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords gabungan Indonesia + Sunda\n",
    "STOPWORDS = set([\n",
    "    'yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'ada', 'dengan',\n",
    "    'untuk', 'pada', 'akan', 'adalah', 'juga', 'bisa', 'nya', 'saja',\n",
    "    'sudah', 'belum', 'lebih', 'agar', 'atau', 'karena', 'jika', 'bila',\n",
    "    'kita', 'kami', 'saya', 'kamu', 'dia', 'mereka', 'anda', 'bapak',\n",
    "    'ibu', 'pak', 'bu', 'pa', 'min', 'kak', 'bang',\n",
    "    # Sunda\n",
    "    'mah', 'teh', 'wae', 'oge', 'kitu', 'kieu', 'nu', 'ka', 'na',\n",
    "    'we', 'eta', 'atuh', 'yeuh', 'euy', 'lah', 'sih', 'ih', 'ah',\n",
    "    'naha', 'kumaha', 'naon', 'iraha', 'dimana', 'keneh', 'pisan',\n",
    "    # Umum chat\n",
    "    'banget', 'bgt', 'aja', 'ajah', 'dong', 'deh', 'yah', 'ya', 'yg',\n",
    "    'jd', 'tp', 'dgn', 'utk', 'krn', 'pd', 'sm', 'dr', 'skrg', 'sdh',\n",
    "])\n",
    "\n",
    "sentimen_labels = ['Negatif', 'Netral', 'Positif']\n",
    "warna_wc = {'Negatif': 'Reds', 'Netral': 'Greys', 'Positif': 'Greens'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.patch.set_facecolor('#F2F2F7')\n",
    "\n",
    "for ax, label in zip(axes, sentimen_labels):\n",
    "    subset = df[df['sentimen'] == label]['teks_bersih']\n",
    "    teks_gabung = ' '.join(subset.tolist())\n",
    "\n",
    "    if len(teks_gabung.strip()) < 10:\n",
    "        ax.text(0.5, 0.5, f'Data {label}\\nterlalu sedikit',\n",
    "                ha='center', va='center', fontsize=12, transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "\n",
    "    wc = WordCloud(\n",
    "        width=500, height=350,\n",
    "        background_color='white',\n",
    "        colormap=warna_wc[label],\n",
    "        stopwords=STOPWORDS,\n",
    "        max_words=60,\n",
    "        min_font_size=10,\n",
    "        prefer_horizontal=0.85,\n",
    "        collocations=False\n",
    "    ).generate(teks_gabung)\n",
    "\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'WordCloud â€” {label}\\n({len(subset)} komentar)',\n",
    "                 fontsize=12, fontweight='bold',\n",
    "                 color=list(WARNA.values())[sentimen_labels.index(label)])\n",
    "\n",
    "plt.tight_layout(pad=1.5)\n",
    "plt.savefig('viz_wordcloud.png', dpi=150, bbox_inches='tight', facecolor='#F2F2F7')\n",
    "plt.show()\n",
    "print('ğŸ’¾ Tersimpan: viz_wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¢ Cell 10 â€” Kata Paling Sering Muncul per Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_kata(df_subset, n=12):\n",
    "    semua_kata = []\n",
    "    for teks in df_subset['teks_bersih']:\n",
    "        kata_list = [k for k in teks.split() if k not in STOPWORDS and len(k) > 2]\n",
    "        semua_kata.extend(kata_list)\n",
    "    return Counter(semua_kata).most_common(n)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.patch.set_facecolor('#F2F2F7')\n",
    "\n",
    "for ax, label in zip(axes, sentimen_labels):\n",
    "    subset = df[df['sentimen'] == label]\n",
    "    top = top_kata(subset)\n",
    "    if not top:\n",
    "        ax.text(0.5, 0.5, 'Tidak ada data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "\n",
    "    kata_list, freq_list = zip(*top)\n",
    "    color = WARNA[label]\n",
    "    ax.set_facecolor('white')\n",
    "    bars = ax.barh(list(reversed(kata_list)), list(reversed(freq_list)),\n",
    "                   color=color, alpha=0.85, edgecolor='white')\n",
    "    ax.set_title(f'Top Kata â€” {label}', fontsize=12, fontweight='bold', color=color)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='y', labelsize=9)\n",
    "    ax.set_xlabel('Frekuensi', fontsize=9)\n",
    "\n",
    "plt.suptitle('Kata Paling Sering Muncul per Sentimen', fontsize=14, fontweight='bold',\n",
    "             color='#1C1C1E', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz_topkata.png', dpi=150, bbox_inches='tight', facecolor='#F2F2F7')\n",
    "plt.show()\n",
    "print('ğŸ’¾ Tersimpan: viz_topkata.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Cell 11 â€” Export Dataset untuk Machine Learning\n",
    "\n",
    "> **Ini bagian utama!** Dataset yang dihasilkan di sini siap digunakan untuk:\n",
    "> - Training model Machine Learning (Naive Bayes, SVM, BERT, dll.)\n",
    "> - Fine-tuning LLM\n",
    "> - Transfer learning / dataset benchmark\n",
    ">\n",
    "> Format yang tersedia: **CSV**, **Excel**, **JSON**, **JSONL** (Hugging Face format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  BUAT DATASET ML\n",
    "#  Setiap baris = satu sampel data berlabel\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Label encoding untuk ML\n",
    "LABEL_MAP = {'Negatif': 0, 'Netral': 1, 'Positif': 2}\n",
    "\n",
    "# Kolom yang dibutuhkan untuk ML\n",
    "kolom_ml = {\n",
    "    'id': range(1, len(df) + 1),\n",
    "    'teks_asli':   df['komentar'].values,\n",
    "    'teks_bersih': df['teks_bersih'].values,\n",
    "    'label':       df['sentimen'].values,             # label string\n",
    "    'label_id':    df['sentimen'].map(LABEL_MAP).values,  # label angka (0/1/2)\n",
    "    'skor':        df['skor'].values,\n",
    "    'kata_kunci':  df['kata_kunci'].values,\n",
    "}\n",
    "\n",
    "# Tambahkan kolom opsional jika ada\n",
    "if 'username' in df.columns:\n",
    "    kolom_ml['username'] = df['username'].fillna('').values\n",
    "if 'waktu' in df.columns:\n",
    "    kolom_ml['waktu'] = df['waktu'].fillna('').values\n",
    "if 'tipe' in df.columns:\n",
    "    kolom_ml['tipe'] = df['tipe'].fillna('').values\n",
    "if 'likes' in df.columns:\n",
    "    kolom_ml['likes'] = pd.to_numeric(df['likes'], errors='coerce').fillna(0).astype(int).values\n",
    "\n",
    "df_ml = pd.DataFrame(kolom_ml)\n",
    "\n",
    "print('ğŸ“¦ Dataset ML siap:')\n",
    "print(f'   Baris   : {len(df_ml)}')\n",
    "print(f'   Kolom   : {list(df_ml.columns)}')\n",
    "print(f'   Label   : {df_ml[\"label\"].value_counts().to_dict()}')\n",
    "print(f'   Label ID: {dict(LABEL_MAP)}')\n",
    "print()\n",
    "display(df_ml[['id', 'teks_asli', 'teks_bersih', 'label', 'label_id', 'skor']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Export ke berbagai format â”€â”€â”€\n",
    "\n",
    "# 1. CSV (format paling umum untuk ML)\n",
    "df_ml.to_csv('dataset_ml.csv', index=False, encoding='utf-8-sig')\n",
    "print('âœ… Tersimpan: dataset_ml.csv')\n",
    "\n",
    "# 2. Excel (untuk review manual)\n",
    "with pd.ExcelWriter('dataset_ml.xlsx', engine='openpyxl') as writer:\n",
    "    df_ml.to_excel(writer, sheet_name='Dataset ML', index=False)\n",
    "    # Sheet ringkasan\n",
    "    ringkasan = pd.DataFrame({\n",
    "        'Label': list(LABEL_MAP.keys()),\n",
    "        'Label ID': list(LABEL_MAP.values()),\n",
    "        'Jumlah': [len(df_ml[df_ml['label'] == l]) for l in LABEL_MAP.keys()],\n",
    "        'Persentase (%)': [round(len(df_ml[df_ml['label'] == l]) / len(df_ml) * 100, 2)\n",
    "                           for l in LABEL_MAP.keys()],\n",
    "    })\n",
    "    ringkasan.to_excel(writer, sheet_name='Ringkasan', index=False)\n",
    "print('âœ… Tersimpan: dataset_ml.xlsx')\n",
    "\n",
    "# 3. JSON (untuk API / Python dict)\n",
    "df_ml.to_json('dataset_ml.json', orient='records', force_ascii=False, indent=2)\n",
    "print('âœ… Tersimpan: dataset_ml.json')\n",
    "\n",
    "# 4. JSONL â€” format Hugging Face / OpenAI fine-tuning\n",
    "with open('dataset_ml.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for _, row in df_ml.iterrows():\n",
    "        record = {\n",
    "            'text':     row['teks_bersih'],\n",
    "            'label':    row['label'],\n",
    "            'label_id': int(row['label_id']),\n",
    "        }\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "print('âœ… Tersimpan: dataset_ml.jsonl (Hugging Face format)')\n",
    "\n",
    "# 5. Split train/test (80/20) â€” siap untuk training\n",
    "from sklearn.model_selection import train_test_split  # pip install scikit-learn jika belum ada\n",
    "\n",
    "try:\n",
    "    df_train, df_test = train_test_split(\n",
    "        df_ml, test_size=0.2, random_state=42,\n",
    "        stratify=df_ml['label']  # proporsional tiap kelas\n",
    "    )\n",
    "    df_train.to_csv('dataset_train.csv', index=False, encoding='utf-8-sig')\n",
    "    df_test.to_csv('dataset_test.csv',  index=False, encoding='utf-8-sig')\n",
    "    print(f'âœ… Train: {len(df_train)} baris â†’ dataset_train.csv')\n",
    "    print(f'âœ… Test : {len(df_test)} baris  â†’ dataset_test.csv')\n",
    "    print(f'   Distribusi train: {df_train[\"label\"].value_counts().to_dict()}')\n",
    "    print(f'   Distribusi test : {df_test[\"label\"].value_counts().to_dict()}')\n",
    "except ImportError:\n",
    "    print('âš ï¸  scikit-learn belum terinstall. Jalankan: !pip install scikit-learn')\n",
    "    print('   Train/test split dilewati.')\n",
    "\n",
    "print('\\nğŸ‰ Export selesai!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Cell 12 â€” Metadata & Dokumentasi Dataset\n",
    "\n",
    "> File `dataset_metadata.json` ini penting agar orang lain (atau model ML kita) tahu\n",
    "> konteks dan cara membaca dataset ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung statistik teks\n",
    "df_ml['panjang_teks'] = df_ml['teks_bersih'].str.len()\n",
    "df_ml['jumlah_kata']  = df_ml['teks_bersih'].str.split().str.len()\n",
    "\n",
    "metadata = {\n",
    "    'nama_dataset': 'SENTALIS â€” Instagram Comment Sentiment Dataset',\n",
    "    'sumber': '@inimahsumedang (Instagram)',\n",
    "    'konteks': 'Komentar postingan evaluasi setahun kepemimpinan Bupati Sumedang',\n",
    "    'bahasa': ['Indonesia', 'Sunda', 'campuran'],\n",
    "    'metode_labeling': 'Rule-based (keyword + frasa, hierarki negatif â†’ positif â†’ netral)',\n",
    "    'catatan': 'Label bersifat estimasi otomatis, belum diverifikasi manusia penuh',\n",
    "    'statistik': {\n",
    "        'total_sampel': int(len(df_ml)),\n",
    "        'distribusi_label': df_ml['label'].value_counts().to_dict(),\n",
    "        'distribusi_label_id': df_ml['label_id'].value_counts().to_dict(),\n",
    "        'rata_panjang_teks': round(df_ml['panjang_teks'].mean(), 1),\n",
    "        'rata_jumlah_kata': round(df_ml['jumlah_kata'].mean(), 1),\n",
    "        'teks_terpendek': int(df_ml['panjang_teks'].min()),\n",
    "        'teks_terpanjang': int(df_ml['panjang_teks'].max()),\n",
    "    },\n",
    "    'skema_label': {\n",
    "        '0 = Negatif': 'Komentar mengandung kritik, kekecewaan, atau sentimen buruk',\n",
    "        '1 = Netral':  'Komentar faktual, ambigu, atau tidak mengandung muatan emosi jelas',\n",
    "        '2 = Positif': 'Komentar mengandung apresiasi, dukungan, atau sentimen baik',\n",
    "    },\n",
    "    'kolom_ml_utama': {\n",
    "        'teks_bersih': 'Input untuk model (sudah di-preprocess)',\n",
    "        'label':       'Target label string (Negatif/Netral/Positif)',\n",
    "        'label_id':    'Target label angka (0/1/2) untuk ML',\n",
    "    },\n",
    "    'file_tersedia': [\n",
    "        'dataset_ml.csv       â€” dataset lengkap',\n",
    "        'dataset_ml.xlsx      â€” dataset + sheet ringkasan',\n",
    "        'dataset_ml.json      â€” format JSON records',\n",
    "        'dataset_ml.jsonl     â€” format Hugging Face/OpenAI',\n",
    "        'dataset_train.csv    â€” 80% untuk training',\n",
    "        'dataset_test.csv     â€” 20% untuk evaluasi',\n",
    "    ],\n",
    "    'rekomendasi_ml_selanjutnya': [\n",
    "        'TF-IDF + Naive Bayes (baseline cepat)',\n",
    "        'TF-IDF + SVM (akurasi lebih baik)',\n",
    "        'IndoBERT fine-tuning (terbaik untuk bahasa Indonesia)',\n",
    "        'Pertimbangkan human annotation untuk meningkatkan kualitas label',\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('dataset_metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('âœ… Tersimpan: dataset_metadata.json')\n",
    "print()\n",
    "print('ğŸ“‹ Ringkasan Metadata:')\n",
    "print(f'   Sumber         : {metadata[\"sumber\"]}')\n",
    "print(f'   Total sampel   : {metadata[\"statistik\"][\"total_sampel\"]}')\n",
    "print(f'   Rata kata/teks : {metadata[\"statistik\"][\"rata_jumlah_kata\"]} kata')\n",
    "print(f'   Distribusi     : {metadata[\"statistik\"][\"distribusi_label\"]}')\n",
    "print()\n",
    "print('â”€â”€â”€ Skema Label â”€â”€â”€')\n",
    "for k, v in metadata['skema_label'].items():\n",
    "    print(f'   {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Cell 13 â€” Download Semua File (Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Kumpulkan semua file output\n",
    "files_to_zip = [\n",
    "    'dataset_ml.csv',\n",
    "    'dataset_ml.xlsx',\n",
    "    'dataset_ml.json',\n",
    "    'dataset_ml.jsonl',\n",
    "    'dataset_train.csv',\n",
    "    'dataset_test.csv',\n",
    "    'dataset_metadata.json',\n",
    "    'viz_distribusi.png',\n",
    "    'viz_wordcloud.png',\n",
    "    'viz_topkata.png',\n",
    "]\n",
    "if os.path.exists('viz_tipe.png'):\n",
    "    files_to_zip.append('viz_tipe.png')\n",
    "\n",
    "zip_name = 'SENTALIS_output.zip'\n",
    "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    for fname in files_to_zip:\n",
    "        if os.path.exists(fname):\n",
    "            zf.write(fname)\n",
    "            print(f'  + {fname}')\n",
    "        else:\n",
    "            print(f'  - {fname} (tidak ditemukan, dilewati)')\n",
    "\n",
    "print(f'\\nğŸ“¦ ZIP berhasil dibuat: {zip_name}')\n",
    "print(f'   Ukuran: {os.path.getsize(zip_name) / 1024:.1f} KB')\n",
    "print()\n",
    "\n",
    "# Auto-download jika di Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_name)\n",
    "    print('â¬‡ï¸  Download dimulai...')\n",
    "except ImportError:\n",
    "    print(f'ğŸ“‚ File tersimpan di direktori saat ini: {os.path.abspath(zip_name)}')\n",
    "    print('   (Jika di Colab, jalankan ulang cell ini setelah semua cell selesai)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ—ºï¸ Langkah Selanjutnya â€” dari Rule-Based ke Machine Learning\n",
    "\n",
    "Dataset yang sudah diexport bisa langsung digunakan untuk:\n",
    "\n",
    "| Pendekatan | Library | File Input |\n",
    "|---|---|---|\n",
    "| **TF-IDF + Naive Bayes** | `sklearn` | `dataset_train.csv` + `dataset_test.csv` |\n",
    "| **TF-IDF + SVM** | `sklearn` | `dataset_train.csv` + `dataset_test.csv` |\n",
    "| **IndoBERT / BERT** | `transformers` | `dataset_ml.jsonl` |\n",
    "| **GPT fine-tuning** | `openai` | `dataset_ml.jsonl` |\n",
    "\n",
    "**Rekomendasi:** Lakukan **human review** minimal pada komentar yang `skor` mendekati 0  \n",
    "(ambiguous) sebelum training model untuk hasil yang lebih akurat.\n",
    "\n",
    "```python\n",
    "# Filter komentar ambigu untuk review manual:\n",
    "df_ambigu = df_ml[df_ml['skor'] == 0]\n",
    "df_ambigu.to_csv('review_manual.csv', index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "*SENTALIS â€” Sentiment Analysis Intelligent System*  \n",
    "*Rule-Based Edition dengan ML Export*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "name": "SENTALIS_RuleBased_MLExport.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
